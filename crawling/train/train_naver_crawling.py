# -*- coding: utf-8 -*-
"""train_naver_crawling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wzihH2D2_wk3NIU02A0gF9k1YdxZ_zGx
"""

# importing necessary libraries

from bs4 import BeautifulSoup
import requests
import urllib
from urllib.request import urlopen
import pandas as pd

# getting train data file from github

train_csv_github_url = "https://raw.githubusercontent.com/forrestpark/adonde.kr/main/data/train/train_sido_name_id.csv"
train = pd.read_csv(train_csv_github_url)

# request url settings

search_base_url = "https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query="

def add_sidosgg_url(sido, name):
  query = sido + " " + name + "역"
  return "https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=" + urllib.parse.quote(query)

# hash tables for standardizing the "sido" names

special_sido2api = {"서울특별시" : "서울", "인천광역시": "인천", "대구광역시": "대구", "대전광역시": "대전",
            "광주광역시": "광주", "부산광역시": "부산", "울산광역시": "울산",
            "세종특별자치시": "세종"}

special_api2sido = {val: key for key, val in special_sido2api.items()}

train

# helper method that adds sido, sgg, name, id values to its respective lists

def make_lists(sido, sgg, name, id):
  sido_list.append(sido)
  sgg_list.append(sgg)
  name_list.append(name)
  id_list.append(id)

# initializing necessary lists
sido_list, sgg_list, name_list, id_list = [], [], [], []
scope = len(train)
count = 0

# for each station in the train station data from github
for station in train.iterrows():
  station = station[1]
  # save sido, name, id values for respective station
  sido = station['sido']
  name = station['station_name']
  id = station['station_id']

  print(sido, name, id)

  # if the sido value is one of the special sido values, do not perform any crawling
  # instead, use the current sido value, duplicate, and save it as the sgg value
  if sido in special_sido2api.values() or sido in special_sido2api.keys():
    make_lists(sido, sido, name, id)
    print("특별/광역시")

  # else (if the sido value is neither of the special sido values)
  # we perform web crawling from NAVER
  else:

    # send web crawling requests
    url = add_sidosgg_url(sido, name)
    res = urllib.request.urlopen(url).read()

    # web crawling from NAVER using BeautifulSoup
    soup = BeautifulSoup(res, 'html.parser')

    # finding the train station locator on NAVER
    div = soup.find("div", "cs_train_pc api_subject_bx")
    if div:
      station_check = div.find("span", "_2STgUbtdHkTkZQZcC-PCzn")
      if station_check.text[-1] == "역":
      # print("station check: ", station_check[0].text)
        ul = div.find("ul", " _277KNiLveQTo9EetAOMaYm fxL5EYHAuxj6vuOcl6V5v")
        crawled_sgg = ul.find("span", "_1wc0Fn-d58aB3U-71FPC-N ")

        # successfully crawled sgg value of the train station
        sgg = crawled_sgg.text.split(" ")[1]
        print(sgg)

    else:
      count += 1

      # alternative way to crawl the sgg value
      crawled_sgg = soup.find("span", "_3hCbH")
      if not crawled_sgg:
        continue

      # successfully crawled sgg value of the train station
      sgg = crawled_sgg.text.split(" ")[0]
      print(sgg)
      # make_lists(sido, sgg, name, id)
    
    # preprocess the web-crawled sgg value
    # get rid of the city suffix
    if sgg[-1] == "시" or sgg[-1] == "군":
      print("pre-sgg: ", sgg)

      # sgg value successfully preprocessed
      sgg = sgg[0:len(sgg)-1]
      print("post-sgg: ", sgg)
      
    # adding the crawled data and related data to respective lists
    make_lists(sido, sgg, name, id)

print(len(name_list), len(id_list), len(sido_list), len(sgg_list))

# exporting the crawled data a
data = {"station_id":id_list, "station_name":name_list, "sido":sido_list, "sgg":sgg_list}
df = pd.DataFrame(data)
df.to_csv("train_id_name_sido_sgg.csv")